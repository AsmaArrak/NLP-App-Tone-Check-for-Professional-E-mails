# Tone Check for Professional Eâ€‘mails
code: https://colab.research.google.com/drive/1zEuFe6gPUOrAVF0cXoJnPshGYYdWUTxK?usp=sharing

## Introduction

**Tone Check** is a web application that helps users assess the emotional tone of draft emails before sending. By pasting your message into the interface, you receive:

- A color-coded risk badge (ðŸ”´Â Risky, âšªÂ Neutral, ðŸŸ¡Â Caution, ðŸŸ¢Â Positive)
- A list of the top three detected emotions with confidence scores

This tool ensures your communications maintain a professional tone and avoid unintended negative impressions.

## Usage

1. **Launch the demo**: Click the Gradio link (or visit the hosted URL) to open the app.
2. **Paste your email**: Enter your draft into the text box.
3. **Choose a model**: Select between the fineâ€‘tuned **DistilBERT** or the **TFâ€‘IDFÂ LogReg** baseline.
4. **Analyze**: Click **Analyze âœ¨** to see the risk badge and top emotions.
5. **Try examples**: Use one-click examples to explore typical scenarios.

<details>
<summary>Example Input</summary>

```
Hi team,

I still havenâ€™t received the report you promised last week. Please send it ASAP.

Thanks,
Jordan
```

**Output**:
- Risk level: ðŸ”´Â Risky
- Top emotions:
  - annoyance: 0.87
  - urgency: 0.64
  - neutral: 0.20
</details>

## Documentation

### Core Components

- **Python & Gradio**: Frontend UI built with Gradio Blocks for ease of deployment and interactivity.
- **GoEmotions Dataset**: 28 emotion labels, 58k Reddit comments annotated for fineâ€‘tuning.
- **Transformers**: `distilbert-base-uncased` fineâ€‘tuned for multiâ€‘label emotion classification.
- **scikit-learn**: TFâ€‘IDF vectorizer + LogisticRegression baseline.


### DistilBERT Details

- **Architecture**: 6-layer, 66Mâ€‘parameter DistilBERT with a 28â€‘way classification head.
- **Training**:
  - Dataset: GoEmotions (train/validation/test splits)
  - Problem Type: `multi_label_classification`
  - Hyperparameters: 3 epochs, batchÂ sizeÂ 8, learning rateÂ 2eâ€‘5, threshold optimized atÂ 0.25
- **Limitations**:
  - May misinterpret sarcasm or complex multi-sentence context.
  - Threshold tuning required for different domains.

### TFâ€‘IDFÂ Baseline Details

- **Features**: Unigram+bigram TFâ€‘IDF (maxÂ 40,000 features)
- **Classifier**: Oneâ€‘vsâ€‘rest LogisticRegression, singleâ€‘label (first emotion tag)
- **Performance**: Trains in seconds on CPU; microâ€‘F1 â‰ˆÂ 0.35 vsÂ 0.53 for DistilBERT


##  Results

| Model                   | microâ€‘F1 | macroâ€‘F1 | subsetâ€‘Acc |
| ----------------------- | -------- | -------- | ---------- |
| TFâ€‘IDFÂ LogReg           | 0.504    | 0.260    | 0.504      |
| DistilBERTÂ (fineâ€‘tuned) | 0.100    | 0.023    | 0.020      |




## Limitations

- **Domain Shift**: GoEmotions is Redditâ€‘based; corporate email style may differ.
- **Multi-sentence Context**: The model processes entire input as one sequence, losing longâ€‘context nuance.
- **Emotion Granularity**: Reducing 28 emotions to 4 buckets may oversimplify subtlety.
- **Bias**: Trained on English Reddit data; may not generalize to other cultures or languages.

---


