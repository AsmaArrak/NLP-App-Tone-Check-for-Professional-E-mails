# Tone Check for Professional Eâ€‘mails
code: https://colab.research.google.com/drive/1zEuFe6gPUOrAVF0cXoJnPshGYYdWUTxK?usp=sharing

## Introduction

**Tone Check** is a web application that helps users assess the emotional tone of draft emails before sending. By pasting your message into the interface, you receive:

- A color-coded risk badge (ðŸ”´Â Risky, âšªÂ Neutral, ðŸŸ¡Â Caution, ðŸŸ¢Â Positive)
- A list of the top three detected emotions with confidence scores

This tool ensures your communications maintain a professional tone and avoid unintended negative impressions.

## Usage

1. **Launch the demo**: Click the Gradio link (or visit the hosted URL) to open the app.
2. **Paste your email**: Enter your draft into the text box.
3. **Choose a model**: Select between the fineâ€‘tuned **DistilBERT** or the **TFâ€‘IDFÂ LogReg** baseline.
4. **Analyze**: Click **Analyze âœ¨** to see the risk badge and top emotions.
5. **Try examples**: Use one-click examples to explore typical scenarios.

<details>
<summary>Example Input</summary>

```
Hi team,

I still havenâ€™t received the report you promised last week. Please send it ASAP.

Thanks,
Jordan
```

**Output**:
- Risk level: ðŸ”´Â Risky
- Top emotions:
  - annoyance: 0.87
  - urgency: 0.64
  - neutral: 0.20
</details>
-------------------------
## Documentation

### Dataset

**Source & Annotations**: I used the GoEmotions dataset, which contains **58,009** Reddit comments manually annotated for **27 emotion categories** plus a **neutral** class. Each comment was labeled by three annotators, and any label agreed on by at least two annotators was retained. The final dataset splits are:

- **Training**: 42,241 examples  
- **Validation**: 5,426 examples  
- **Test**: 5,427 examples  

I converted each list of emotion tags into a **28-dimensional binary vector** (multi-hot) for model fineâ€‘tuning.

### Baseline Model (TFâ€‘IDF + Logistic Regression)

- **Library**: `scikit-learn`  
- **Vectorizer**: `TfidfVectorizer` with a vocabulary of the top 40,000 unigrams and bigrams by frequency.
- **Classifier**: `LogisticRegression` (one-vs-rest) trained on the *first* emotion tag for a single-label baseline.  
- **Performance**: CPU training in under 30 seconds.  

### Transformer Model (DistilBERT)

- **Library**: `transformers` from HuggingÂ Face  
- **Backbone**: `distilbert-base-uncased` (66 million parameters) pre-trained on general English corpora.
- **Fineâ€‘tuning**:
  - **Task**: Multi-label classification with **28 output heads** (sigmoid activations).  
  - **Hyperparameters**: 3 epochs, batch sizeÂ 8, learning rateÂ 2e-5, weight decayÂ 0.01.  
  - **Compute**: Fineâ€‘tuning on a single NVIDIA T4 GPU in approximately 12 minutes.  
  - **Thresholding**: The optimal probability threshold of **0.25** was selected by maximizing microâ€‘F1 on the validation split.



          --------------------------------


##  Results

| Model                   | microâ€‘F1 | macroâ€‘F1 | subsetâ€‘Acc |
| ----------------------- | -------- | -------- | ---------- |
| TFâ€‘IDFÂ LogReg           | 0.504    | 0.260    | 0.504      |
| DistilBERTÂ (fineâ€‘tuned) | 0.100    | 0.023    | 0.020      |




## Limitations

- **Domain Shift**: GoEmotions is Redditâ€‘based; corporate email style may differ.
- **Multi-sentence Context**: The model processes entire input as one sequence, losing longâ€‘context nuance.
- **Emotion Granularity**: Reducing 28 emotions to 4 buckets may oversimplify subtlety.
- **Bias**: Trained on English Reddit data; may not generalize to other cultures or languages.

---


