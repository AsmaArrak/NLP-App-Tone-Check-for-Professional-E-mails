# Tone Check for Professional Eâ€‘mails

## Introduction

**Tone Check** is a web application that helps users assess the emotional tone of draft emails before sending. By pasting your message into the interface, you receive:

- A color-coded risk badge (ðŸ”´Â Risky, âšªÂ Neutral, ðŸŸ¡Â Caution, ðŸŸ¢Â Positive)
- A list of the top three detected emotions with confidence scores

This tool ensures your communications maintain a professional tone and avoid unintended negative impressions.

## Usage

1. **Launch the demo**: Click the Gradio link (or visit the hosted URL) to open the app.
2. **Paste your email**: Enter your draft into the text box.
3. **Choose a model**: Select between the fineâ€‘tuned **DistilBERT** or the **TFâ€‘IDFÂ LogReg** baseline.
4. **Analyze**: Click **Analyze âœ¨** to see the risk badge and top emotions.
5. **Try examples**: Use one-click examples to explore typical scenarios.

<details>
<summary>Example Input</summary>

```
Hi team,

I still havenâ€™t received the report you promised last week. Please send it ASAP.

Thanks,
Jordan
```

**Output**:
- Risk level: ðŸ”´Â Risky
- Top emotions:
  - annoyance: 0.87
  - urgency: 0.64
  - neutral: 0.20
</details>

## Documentation

### Core Components

- **Python & Gradio**: Frontend UI built with Gradio Blocks for ease of deployment and interactivity.
- **GoEmotions Dataset**: 28 emotion labels, 58k Reddit comments annotated for fineâ€‘tuning.
- **Transformers**: `distilbert-base-uncased` fineâ€‘tuned for multiâ€‘label emotion classification.
- **scikit-learn**: TFâ€‘IDF vectorizer + LogisticRegression baseline.

### Data Flow

```mermaid
flowchart TD
  A[User Input] --> B[Gradio Interface]
  B --> C{Model Choice}
  C --> D[DistilBERT Pipeline]
  C --> E[TF-IDFÂ Baseline]
  D --> F[Sigmoid â†’ Probabilities]
  E --> F[Singleâ€‘label Prediction]
  F --> G[ThresholdÂ (0.25) â†’ Binary Labels]
  G --> H[Risk Bucket & Top Emotions]
  H --> I[Display Badge & JSON]
```

### DistilBERT Details

- **Architecture**: 6-layer, 66Mâ€‘parameter DistilBERT with a 28â€‘way classification head.
- **Training**:
  - Dataset: GoEmotions (train/validation/test splits)
  - Problem Type: `multi_label_classification`
  - Hyperparameters: 3 epochs, batchÂ sizeÂ 8, learning rateÂ 2eâ€‘5, threshold optimized atÂ 0.25
- **Limitations**:
  - May misinterpret sarcasm or complex multi-sentence context.
  - Threshold tuning required for different domains.

### TFâ€‘IDFÂ Baseline Details

- **Features**: Unigram+bigram TFâ€‘IDF (maxÂ 40,000 features)
- **Classifier**: Oneâ€‘vsâ€‘rest LogisticRegression, singleâ€‘label (first emotion tag)
- **Performance**: Trains in seconds on CPU; microâ€‘F1 â‰ˆÂ 0.35 vsÂ 0.53 for DistilBERT

### Experiments

| Model            | microâ€‘F1 | macroâ€‘F1 | subsetâ€‘Acc |
|------------------|----------|----------|------------|
| TFâ€‘IDFÂ LogReg    | 0.350    | 0.280    | 0.200      |
| DistilBERTÂ (ft)  | 0.535    | 0.462    | 0.318      |
| Zeroâ€‘shot NLI    | 0.480    | 0.410    | 0.290      |

## Contributions

- Fineâ€‘tuned a pre-trained DistilBERT on GoEmotions for professional tone detection.
- Integrated a dualâ€‘model UI (Transformer & TFâ€‘IDF baseline) for sideâ€‘byâ€‘side comparison.
- Built a polished Gradio interface with risk badges, demo examples, and mobileâ€‘friendly layout.
- Performed threshold optimization and multiâ€‘metric evaluation.

## Limitations

- **Domain Shift**: GoEmotions is Redditâ€‘based; corporate email style may differ.
- **Multi-sentence Context**: The model processes entire input as one sequence, losing longâ€‘context nuance.
- **Emotion Granularity**: Reducing 28 emotions to 4 buckets may oversimplify subtlety.
- **Bias**: Trained on English Reddit data; may not generalize to other cultures or languages.

---

*For questions or to view the code, visit [GitHub repo link here].*

