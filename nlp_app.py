# -*- coding: utf-8 -*-
"""NLP App

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zEuFe6gPUOrAVF0cXoJnPshGYYdWUTxK

## Setup
"""

!pip -q install transformers datasets scikit-learn gradio accelerate --upgrade

import os, json, time, inspect, torch, numpy as np, pandas as pd
from datasets import load_dataset, DatasetDict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, f1_score, accuracy_score
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                          TrainingArguments, Trainer, pipeline)
import gradio as gr

os.environ["WANDB_DISABLED"] = "true"
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on:", device)

"""## Using the GoEmotions dataset"""

ds: DatasetDict = load_dataset("go_emotions")          # train/validation/test
# Accurate label list straight from the dataset schema
LABELS = ds["train"].features["labels"].feature.names      # length == 29
print("Loaded", len(LABELS), "emotion labels:", LABELS[:10], "...")


RISKY   = {"anger","annoyance","disgust","sarcasm",
           "disapproval","embarrassment","sadness"}
POS_OK  = {"admiration","gratitude","optimism","excitement","approval"}
NEUTRAL = {"neutral"}

def bucket(pred_vec):
    active = [LABELS[i] for i,v in enumerate(pred_vec) if v==1]
    if any(a in RISKY   for a in active):  return "Risky "
    if any(a in POS_OK  for a in active):  return "Positive "
    if any(a in NEUTRAL for a in active):  return "Neutral "
    return "Caution "

"""# # Baseline  TF‑IDF + LogReg"""

def first_emotion(label_list):
    return label_list[0] if label_list else 28  # 28 == 'neutral'

X_train, y_train = ds["train"]["text"], [first_emotion(l) for l in ds["train"]["labels"]]
X_test , y_test  = ds["test"]["text"] , [first_emotion(l) for l in ds["test"]["labels"]]

tfidf = TfidfVectorizer(max_features=40_000, ngram_range=(1,2))
clf   = LogisticRegression(max_iter=1000, n_jobs=-1).fit(tfidf.fit_transform(X_train), y_train)

pred = clf.predict(tfidf.transform(X_test))
acc  = round(accuracy_score(y_test, pred), 3)
print("TF‑IDF LogReg  accuracy:", acc)
print(classification_report(y_test, pred, target_names=LABELS))

"""## Fine‑tune DistilBERT"""

from datasets import Sequence, Value
tok = AutoTokenizer.from_pretrained("distilbert-base-uncased")
def tok_func(b): return tok(b["text"], truncation=True, padding="max_length", max_length=128)
ds_tok = ds.map(tok_func, batched=True)

# Convert labels to 29‑dim float vector
def to_float(example):
    example["labels"] = [float(i in example["labels"]) for i in range(29)]
    return example
ds_tok = ds_tok.map(to_float)

ds_tok = ds_tok.cast_column("labels", Sequence(Value(dtype="float32"), length=29))
ds_tok.set_format(type="torch", columns=["input_ids","attention_mask","labels"])

# Model
model = AutoModelForSequenceClassification.from_pretrained(
            "distilbert-base-uncased",
            num_labels=29,
            problem_type="multi_label_classification").to(device)


base = dict(output_dir="./emo_model",
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            num_train_epochs=3,
            evaluation_strategy="epoch",
            save_strategy="no",
            report_to="none")
TA = TrainingArguments(**{k:v for k,v in base.items()
                          if k in inspect.signature(TrainingArguments).parameters})

Trainer(model=model, args=TA,
        train_dataset=ds_tok["train"],
        eval_dataset=ds_tok["validation"]).train()

model.save_pretrained("./emo_model"); tok.save_pretrained("./emo_model")
print("✅ DistilBERT fine‑tuned and saved → ./emo_model")

# DistilBERT pipeline
distil_pipe = pipeline("text-classification",
                       model="./emo_model",
                       tokenizer="./emo_model",
                       function_to_apply="sigmoid",
                       top_k=None,
                       device=0 if device=="cuda" else -1)

#TF‑IDF baseline as a simple lambda
tfidf_pipe = lambda txt: clf.predict(tfidf.transform([txt]))[0]

def analyse(text, choice):
    if choice=="DistilBERT":
        raw = distil_pipe(text)[0]
        vec = [1 if d["score"]>0.5 else 0 for d in raw]
    else:
        idx = tfidf_pipe(text)
        raw = [{"label": LABELS[idx], "score":1.0}]
        vec = [1 if i==idx else 0 for i in range(29)]
    risk = bucket(vec)
    top3 = sorted(raw, key=lambda x: x["score"], reverse=True)[:3]
    return risk, {d["label"]: round(d["score"],2) for d in top3}

import gradio as gr, datetime, html

THRESHOLD = 0.25
css = """
#container {max-width:780px;margin:auto;font-family:"Helvetica Neue",Helvetica,Arial,sans-serif;}
.risk-badge{display:inline-block;padding:6px 14px;border-radius:20px;font-weight:700;color:#fff;margin-bottom:6px}
.risk-red{background:#e03131}.risk-green{background:#2b8a3e}.risk-yellow{background:#e67700}.risk-gray{background:#6c757d}
"""

def badge(text):
    cls = ("risk-red"   if "Risky"    in text else
           "risk-green" if "Positive" in text else
           "risk-yellow"if "Caution"   in text else
           "risk-gray")
    return f'<span class="risk-badge {cls}">{html.escape(text)}</span>'

def analyse_ui(text, model_choice):
    if "DistilBERT" in model_choice:
        raw = distil_pipe(text)[0]
        # map LABEL_i to the actual emotion name ( I am not sure if I could have done this in the training)
        mapped = []
        for item in raw:
            idx = int(item["label"].split("_")[1])
            name = LABELS[idx]
            mapped.append({"label": name, "score": item["score"]})
        raw = mapped
        vec = [1 if d["score"] > THRESHOLD else 0 for d in raw]
    else:
        # TF‑IDF baseline (single label)
        idx = tfidf_pipe(text)
        raw = [{"label": LABELS[idx], "score": 1.0}]
        vec = [1 if i == idx else 0 for i in range(len(LABELS))]

    tone = bucket(vec)
    top3 = sorted(raw, key=lambda x: x["score"], reverse=True)[:3]
    emo_dict = {d["label"]: round(d["score"], 2) for d in top3}
    return badge(tone), emo_dict

with gr.Blocks(css=css, title="Tone Check for Professional E‑mails") as demo:
    with gr.Column(elem_id="container"):
        gr.Markdown("## ✉️ Tone Check for Professional E‑mails")
        gr.Markdown("Detect the emotional tone of your message and get a quick risk assessment before you hit <kbd>Send</kbd>.")

        inp = gr.Textbox(lines=8, label="Draft e‑mail", placeholder="Paste or type your e‑mail...")

        model_sel = gr.Radio(
            ["DistilBERT (fine‑tuned)", "TF‑IDF baseline"],
            value="DistilBERT (fine‑tuned)",
            label="Model"
        )

        risk_html = gr.HTML(label="Risk level", value=badge("Neutral ⚪"))
        emo_json  = gr.JSON(label="Top emotions (score)")

        btn = gr.Button("Analyze ✨", variant="primary")
        btn.click(analyse_ui, [inp, model_sel], [risk_html, emo_json])

        gr.Examples([
            "Hi John,\n\nI STILL have not received the report you promised last week.\nPlease send it ASAP.\n\nThanks.",
            "Hey team  Great job on the release today! Let’s keep up the momentum.",
            "I’m sorry for the confusion earlier; I’ll make sure the numbers are corrected before tomorrow’s meeting."
        ], inp, label="Try an example:")



demo.launch(share=True)

"""## evaluation"""

from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support
import numpy as np, pandas as pd, torch


def pipe_probs(texts, batch=32):
    raw = distil_pipe(texts, batch_size=batch)
    return np.array([[p["score"] for p in ex] for ex in raw])


prob_val = pipe_probs(ds["validation"]["text"][:4])
NUM_LABELS = prob_val.shape[1]
LABELS     = ds["train"].features["labels"].feature.names
if len(LABELS) < NUM_LABELS:
    LABELS += [f"extra_{i}" for i in range(NUM_LABELS-len(LABELS))]

prob_val = pipe_probs(ds["validation"]["text"])
gold_val = np.array([[1 if i in labs else 0 for i in range(NUM_LABELS)]
                     for labs in ds["validation"]["labels"]])

best_thr, best_f1 = 0.3, 0
for thr in np.linspace(0.05, 0.5, 10):
    f1 = f1_score(gold_val, (prob_val > thr).astype(int), average="micro")
    if f1 > best_f1:
        best_thr, best_f1 = thr, f1
print(f"Optimal threshold on validation: {best_thr:.2f} (micro‑F1 {best_f1:.3f})")


prob_test = pipe_probs(ds["test"]["text"])
gold_bin  = np.array([[1 if i in labs else 0 for i in range(NUM_LABELS)]
                      for labs in ds["test"]["labels"]])
pred_bin  = (prob_test > best_thr).astype(int)

micro  = f1_score(gold_bin, pred_bin, average="micro")
macro  = f1_score(gold_bin, pred_bin, average="macro")
subset = accuracy_score(gold_bin, pred_bin)

print("\n DistilBERT on GoEmotions **test**")
print(f" • micro‑F1   : {micro:.3f}")
print(f" • macro‑F1   : {macro:.3f}")
print(f" • subset‑Acc : {subset:.3f}")

#per‑class precision / recall / F1 (first 10 rows)
prec, rec, f1, _ = precision_recall_fscore_support(
        gold_bin, pred_bin, average=None, zero_division=0)
per_cls = pd.DataFrame({"precision":prec, "recall":rec, "f1":f1},
                       index=LABELS).round(3)
display(per_cls.head(10))